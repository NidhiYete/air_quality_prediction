{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-09T19:37:32.026277900Z",
     "start_time": "2024-02-09T19:37:29.802351700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import aqi\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.templates.default = \"plotly_white\"\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "url_template = \"https://api.openaq.org/v2/measurements?country=IN&location_id={}&date_from={}&date_to={}&limit={}&page={}&offset=0&sort=asc&radius=1000&order_by=datetime\"\n",
    "headers = {\"accept\": \"application/json\"}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ce54b25b37078a0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "limit = 1000  # Set your desired limit\n",
    "locationId = 407\n",
    "# Set the initial date range\n",
    "current_date_from = datetime(2023, 1, 1)\n",
    "current_date_to = datetime(2023, 1, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "936e47012cca97c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set the end date for the loop\n",
    "end_date = datetime(2023, 12, 31)\n",
    "response_list = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c42539b2a74943f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "page = 1\n",
    "while current_date_from < end_date:\n",
    "    date_from_str = current_date_from.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    next_month = current_date_from + relativedelta(months=1)\n",
    "    date_to_str = next_month.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    url = url_template.format(locationId, date_from_str, date_to_str, limit, page)\n",
    "    print(\"url : \"+url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the \"found\" value from the JSON content\n",
    "        json_response = response.json()\n",
    "        found_value_str = str(json_response[\"meta\"][\"found\"])\n",
    "        limit_value_str = json_response[\"meta\"][\"limit\"]\n",
    "        result_count = json_response['results']\n",
    "        response_list.extend(json_response['results'])\n",
    "        #print(json_response)\n",
    "        limit_value = int(limit_value_str)\n",
    "        # Remove \">\" and convert to numeric type\n",
    "        if result_count:\n",
    "            if '>' in found_value_str:\n",
    "                found_value = int(found_value_str.replace('>', ''))\n",
    "            else:\n",
    "                found_value = int (found_value_str)\n",
    "        else:\n",
    "            found_value = 0\n",
    "        # Print \"more\" or \"done\" based on the limit\n",
    "        if found_value > 0:\n",
    "            print(\"more\")\n",
    "            #print(response.text)\n",
    "            page += 1  # Increment the page number for the next iteration\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(\"updating date - as page limit for date range is met.\")\n",
    "            # Update date range for the next iteration\n",
    "            current_date_from = next_month\n",
    "            page = 1\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        break  # Exit the loop on error"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc512c24d47d48f7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#creating an empty dataframe\n",
    "# Create an empty dictionary to store DataFrames for each locationId\n",
    "dfs = {}\n",
    "\n",
    "# Loop through response_data and extract relevant information\n",
    "for entry in response_list:\n",
    "    location_id = entry[\"locationId\"]\n",
    "    date_utc = entry[\"date\"][\"utc\"]\n",
    "    location = entry[\"location\"]\n",
    "    coordinates = entry[\"coordinates\"]\n",
    "    country = entry[\"country\"]\n",
    "    city = entry[\"city\"]\n",
    "    parameter = entry[\"parameter\"]\n",
    "    value = entry[\"value\"]\n",
    "\n",
    "    # Check if the locationId exists in the dictionary\n",
    "    if location_id not in dfs:\n",
    "        # If the locationId is not in the dictionary, create a new DataFrame for it\n",
    "        dfs[location_id] = pd.DataFrame({\n",
    "            'locationId': [location_id],\n",
    "            'date_utc': [date_utc],\n",
    "            'location': [location],\n",
    "            'coordinates': [coordinates],\n",
    "            'country': [country],\n",
    "            'city': [city],\n",
    "            parameter: [value]\n",
    "        })\n",
    "    else:\n",
    "        # If location_id and date_utc combination already exists in the DataFrame, update the corresponding parameter value\n",
    "        if any((dfs[location_id]['date_utc'].eq(date_utc))):\n",
    "            dfs[location_id].loc[dfs[location_id]['date_utc'].eq(date_utc), parameter] = value\n",
    "        else:\n",
    "            # Create a new row for the combination of location_id and date_utc\n",
    "            dfs[location_id] = pd.concat([dfs[location_id], pd.DataFrame({\n",
    "                'locationId': [location_id],\n",
    "                'date_utc': [date_utc],\n",
    "                'location': [location],\n",
    "                'coordinates': [coordinates],\n",
    "                'country': [country],\n",
    "                'city': [city],\n",
    "                parameter: [value]\n",
    "            })])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddd717d8eb16bf33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine all DataFrames into a single DataFrame (if needed)\n",
    "df_combined = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "# Reset the index of the combined DataFrame\n",
    "df_combined.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e18192c2dae91c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#target folder within project\n",
    "target_folder='target'\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "excel_file_path = os.path.join(target_folder, f'output_{timestamp}.xlsx')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47e12f6af8ddb166"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_combined.to_excel(excel_file_path, index=False)\n",
    "print('Excel File generated Successfully')\n",
    "print('program exit')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ac20656912097d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get a list of all Excel files in the folder\n",
    "excel_files = [file for file in os.listdir(target_folder) if file.endswith('.xlsx')]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0580920f322c0d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create an empty list to store DataFrames\n",
    "aqs = []\n",
    "\n",
    "# Loop through each Excel file and read it into a DataFrame\n",
    "for excel_file in excel_files:\n",
    "    file_path = os.path.join(target_folder, excel_file)\n",
    "    aq = pd.read_excel(file_path)\n",
    "    aqs.append(aq)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a5bf20f3d456aa1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_aq = pd.concat(aqs, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff559c8414e1d25"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read Excel file\n",
    "combined_aq.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8288962e42600417"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a new Excel file\n",
    "combined_excel_path = os.path.join(target_folder, 'air_quality.xlsx')\n",
    "combined_aq.to_excel(combined_excel_path, index=False)\n",
    "\n",
    "print('Combined Excel File generated successfully.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b31f6ee8284587b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1d50dfd5c6bf2865"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "10bc4f0b68e87668"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "84bfa90daddf5f10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
